---
title: 'PEC4: Predicción de dolencias cardiacas a partir de un electrocardiograma'
author: "Demetrio Muñoz Alvarez"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
  pdf_document:
    number_sections: false
    toc: true
    toc_depth: 3
  word_document:
    toc: true
    toc_depth: '3'
params:
  mydata: ECGCvdata.csv
  seed_pec4: 12345
  knn_values: !r c(1, 3, 5, 7, 11)
  p.train: !r 0.67
bibliography: scholar.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL, cache=TRUE)
options(width=90)
```

```{r working directory, include = FALSE}
# Establecemos el directorio de trabajo.
setwd("C:/Users/Deme/Desktop/Master/Machine Learning/pec4")
```

```{r load_libraries, include=FALSE}
library(corrplot)
library(knitr)
library(dplyr)
library(class)
library(ggplot2)
library(gmodels)
library(caret)
library(ROCR)
library(e1071)
library(kernlab)
library(keras)
library(C50)
library(randomForest)
```

```{r dinamic_asigns, include = FALSE}
data <- params$mydata
seed <- params$seed_pec4
k_values <- params$knn_values
train_part<-params$p.train
```

\newpage

# Introducción

```{r load_data}
dataset <- read.csv("ECGCvdata.csv") # Cargamos nuestro conjunto de datos. 
```

En esta última actividad evaluable, se realizará un análisis predictivo del tipo de dolencia cardíaca utilizando los datos de los pacientes recopilados en el conjunto de datos `r params$mydata`. Para analizar estas dolencias, se implementarán distintos algoritmos: **k-Nearest Neighbour, Naive Bayes, Artificial Neural Network, Support Vector Machine, Árbol de Decisión y Random Forest**.

Este informe se dividirá en tres partes. La primera parte mostrará un análisis descriptivo del conjunto de datos. Además, se tratarán los datos en busca de valores faltantes y se procederá a su eliminación. La segunda parte consistirá en dividir el conjunto de datos en datos de entrenamiento y prueba para luego entrenar los distintos algoritmos. Por último, se compararán los modelos y se concluirá cuál o cuáles son los mejores para predecir las dolencias cardíacas.

# Exploración y tratamiento de los datos

El conjunto de datos `r params$mydata` esta formado por `r nrow(dataset)` observaciones y `r ncol(dataset)` variables.

En un primer contacto, mostramos las primeras y las últimas seis entradas. En este caso, se elimina la columna **"RECORD"** del conjunto de datos, ya que no aporta información útil para nuestros posteriores análisis. Por último, mostramos el resumen de las diez primeras entradas. 

```{r data_exploration}
head(dataset) # Mostramos las primeras entradas del conjunto de datos. 
tail(dataset) # Mostramos las últimas entradas del conjunto de datos.
dataset <- dataset[,-1] # Eliminamos la columna "RECORD" ya que no aporta informacion útil a nuestros datos.
summary(dataset[,1:10]) # Mostramos el un resumen de las 10 primeras entradas. 
```

La siguiente tabla muestra el número de casos para cada tipo de dolencia: 

```{r table_1, echo = FALSE}
kable(as.data.frame(table(dataset$ECG_signal)),
      col.names= c("ECG_signal", "Frecuencia"),
      align= "cc") # Contamos la frecuencia de cada tipo de afección. 
```

Si analizamos la estructura de los datos, podemos observar que casi todas las variables son de tipo numérico, excepto la variable que muestra las clases, que es de tipo categórica. En algunas de estas variables, observamos valores faltantes que trataremos en el siguiente paso.

```{r data_structure}
str(dataset) # Estructura de las variables. 
```
Los valores faltantes se muestran en la siguiente tabla: 

```{r NaN}
valores_nan <- table(is.na(dataset)) # Contamos y mostramos los valores "NaN"
valores_nan
dataset_nan <- dataset[, colSums(is.na(dataset)) == 0]
```

Como indica el enunciado de la actividad, vamos a tratar los valores faltantes eliminando aquellas variables que presenten algún valor 'NaN'. Una vez eliminados los datos faltantes nuestro conjunto de datos muestra `r nrow(dataset_nan)` observaciones y `r ncol(dataset_nan)` variables.

En la siguiente gráfica, observamos la distribución de las variables. Las variables no se distribuyen de forma uniforme y se observan valores atípicos.  

```{r boxplot_1, echo=FALSE}
boxplot(dataset_nan[,-46], names = NULL, col = c("cyan"),
        main = "Distribución de las variables",
        xlab = "Variables")
```

## Normalización

Con este conjunto de datos, vamos a realizar los algoritmos, pero antes, como hemos observado anteriormente, algunas variables muestran números enteros y otras números decimales. Además, la escala de las variables es diferente. Por ello, se va a proceder a hacer una normalización de los datos. Para realizar esta normalización, se va a crear una función que se aplicará al conjunto de datos siguiendo la formula: $$Normalización = (x - min(x)) / (max(x) - min(x))$$

```{r function_normalization, echo=FALSE}
normalizar <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
```

```{r normalization}
ECG_signal <- dataset_nan[,46] # Extraemos la columna de clases antes de normalizar los datos. 
dataset_norm <- as.data.frame(lapply(dataset_nan[1:45], normalizar)) # Aplicamos la funcion 'lapply()' para aplicar la normalización a nuestras variables.
data_norm <- data.frame(ECG_signal, dataset_norm) # Creamos el conjunto de datos normalizado. 
```

Con los datos normalizados, realizamos una exploración visual de nuestros datos. Examinamos la correlación entre las variables y presentamos otro gráfico de cajas con los datos normalizados. 

```{r corr, echo=FALSE}
cor_matrix <- cor(data_norm[,2:46])
corrplot(cor_matrix, tl.cex = 0.5)
```
```{r boxplot_2, echo=FALSE}
boxplot(data_norm[,-1], names = NULL, col = c("cyan"),
        main = "Distribución de las variables normalizadas",
        xlab = "Variables", ylab = "Valor normalizado")
```

# Partición de los datos en entrenamiento/prueba

El conjunto de datos resultante de la exploración y manejo de los datos se dividirá en dos partes: una para el conjunto de entrenamiento y otra para el conjunto de prueba. Para el conjunto de entrenamiento, dividiremos los datos en un `r train_part*100`% de las observaciones totales, mientras que el conjunto de prueba contendrá el `r 100 - (train_part*100)`% de las observaciones restantes.


```{r train_test_algot}
set.seed(seed)

# Extraemos los indices de una muestra aleatoria de la filas de los datos
indices <- sample(1:nrow(data_norm), size = nrow(data_norm), replace = FALSE)
# Tamaño del conjunto de entrenamiento
train_size <- round(train_part * nrow(data_norm))

# Dividimos los datos en entrenamiento/prueba.
train_data <- data_norm[indices[1:train_size], ]
test_data <- data_norm[indices[(train_size + 1):nrow(data_norm)], ] # Conjunto de prueba (33%).

# Extraemos las labels/etiquetas.
train_label <- train_data$ECG_signal
test_label <- test_data$ECG_signal
```

# Aplicación de Algortimos

En esta sección, exploraremos distintos algoritmos para la clasificación de nuestros datos. Utilizaremos la misma división de los datos para cada algoritmo y seguiremos las directrices del libro de referencia [@lantz2019machine] para implementar los modelos. Los algoritmos a explorar son: 

1. k-Nearest Neighbour
2. Naive Bayes
3. Artificial Neural Network
4. Support Vector Machine
5. Árbol de Clasificación
6. Random Forest

La implementación de estos algoritmos la realizaremos íntegramente con R y RStudio. 

## k-Nearest Neighbour (kNN)

Para implementar el algoritmo **k-Nearest Neighbors (kNN)**, utilizamos el paquete *`class`* [@class] con la función `knn()` y exploraremos los valores de k = `r k_values`. Posteriormente, se presentarán las matrices de confusión para cada modelo utilizando el paquete *'caret'* [@caret].


```{r knn}
set.seed(seed)

# Listas para almacenar los modelos y las matrices resultantes. 
models <- list()
confusion_matrices <- list()

for (k in k_values) {
  model_name <- paste("model_knn_k", k, sep = "")
  # Entrenamiento del modelo con los datos establecidos para cada valor de 'k':
  model <- knn(train = train_data[,-1], test = test_data[,-1], cl = train_data$ECG_signal, k = k)
  # Almacenamos cada modelo resultante de cada valor de 'k':
  models[[model_name]] <- model
  
  # Calculamos la matriz de confusión y almacenamos:
  cm <- confusionMatrix(data = model, reference = as.factor(test_data$ECG_signal))
  confusion_matrices[[model_name]] <- cm 
}
# Imprimimos las matrices de confusión:
print(confusion_matrices)
```

```{r tabla_2_knn, echo=FALSE}
# Extraemos los valores seleccionados de las matrices para mostrarlos en una tabla:
# Dataframe para almacenar los tabla_2
tabla_2 <- data.frame(Modelo = k_values)
accuracy <- numeric(length(k_values))
kappa <- numeric(length(k_values))
error_rate <- numeric(length(k_values))
sensitivity_aff <- numeric(length(k_values))
sensitivity_arr <- numeric(length(k_values))
sensitivity_chf <- numeric(length(k_values))
sensitivity_nsr <- numeric(length(k_values))
# Iterar a través de los diferentes valores de k
for (i in 1:length(k_values)) {
  k <- k_values[i]
  confusion_matrix <- confusion_matrices[[paste("model_knn_k", k, sep = "")]]
  accuracy[i] <- confusion_matrix$overall["Accuracy"]
  kappa[i] <- confusion_matrix$overall["Kappa"]
  error_rate[i] <- 1 - accuracy[i]
  sensitivity_aff[i] <- confusion_matrix$byClass[1]
  sensitivity_arr[i] <- confusion_matrix$byClass[2,1]
  sensitivity_chf[i] <- confusion_matrix$byClass[3,1]
  sensitivity_nsr[i] <- confusion_matrix$byClass[4,1]
}
tabla_2$Modelo <- paste("kNN k = ", tabla_2$Modelo, sep = "")
tabla_2$Accuracy <- round(accuracy, 3)
tabla_2$Kappa <- round(kappa, 3)
tabla_2$Error_rate <- round(error_rate, 3)
tabla_2$Sensitivity_AFF <- round(sensitivity_aff, 3)
tabla_2$Sensitivity_ARR <- round(sensitivity_arr, 3)
tabla_2$Sensitivity_CHF <- round(sensitivity_chf, 3)
tabla_2$Sensitivity_NSR <- round(sensitivity_nsr, 3)
# Ordenamos la tabla por el parámetro "Accuracy" de forma descendente.
tabla_2 <- tabla_2[order(-tabla_2$Accuracy), ]
# Imprimimos la tabla:
kable(tabla_2, caption = "Comparación de Modelos ('kNN')", format = "markdown")
```

De todos los valores de **k** (`r k_values`), el modelo con el valor de `r tabla_2$Modelo[1]` tiene la mejor precisión y valor de kappa, con un `r tabla_2$Accuracy[1]*100`% y `r tabla_2$Kappa[1]*100`%, respectivamente. Además, presenta el menor porcentaje de error con un `r tabla_2$Error_rate[1]*100`%.

También observamos en la tabla los valores de sensibilidad de cada clase. El modelo con las mejores métricas tiene un valor de sensibilidad del `r tabla_2$Sensitivity_AFF[1]*100`% para la clase AFF, `r tabla_2$Sensitivity_ARR[1]*100`% para la clase ARR, `r tabla_2$Sensitivity_CHF[1]*100`% para la clase CHF y `r tabla_2$Sensitivity_NSR[1]*100`% para la última clase NSR.

## Naive Bayes

En este apartado, implementamos el algoritmo de **Naive Bayes** con el paquete *`e1071`* [@e1071] y visualizaremos las matrices de confusión con el paquete *'caret'*. Exploraremos la opción de activar (laplace = 1) o no activar (laplace = 0) **Laplace**.

```{r nb}
set.seed(seed)

# Modelo Nain Bayes con el valor laplace = 0.
naive_model_1 <- naiveBayes(train_data, train_data$ECG_signal, laplace=0)
naive_test_pred_1 <- predict(naive_model_1, test_data)
confusion_matrix_naive_1 <- confusionMatrix(naive_test_pred_1, as.factor(test_label))
print(confusion_matrix_naive_1)

# Modelo Nain Bayes con el valor laplace = 1.
naive_model_2 <- naiveBayes(train_data, train_data$ECG_signa, laplace=1)
naive_test_pred_2 <- predict(naive_model_2, test_data)
confusion_matrix_naive_2 <- confusionMatrix(naive_test_pred_2, as.factor(test_label))
print(confusion_matrix_naive_2)

```

```{r tabla_3_NainBayes, echo=FALSE}
# Creamos la tabla para los modelos de Naive Bayes.
tabla_3 <- data.frame(Modelo = c("NB Laplace = 0", "NB Laplace =1"))

accuracy_naive_1 <- confusion_matrix_naive_1$overall["Accuracy"]
kappa_naive_1 <- confusion_matrix_naive_1$overall["Kappa"]
error_rate_naive_1 <- 1 - accuracy_naive_1

accuracy_naive_2 <- confusion_matrix_naive_2$overall["Accuracy"]
kappa_naive_2 <- confusion_matrix_naive_2$overall["Kappa"]
error_rate_naive_2 <- 1 - accuracy_naive_2

tabla_3$Accuracy <- round(c(accuracy_naive_1, accuracy_naive_2), 3)
tabla_3$Kappa <- round(c(kappa_naive_1, kappa_naive_2), 3)
tabla_3$Error_rate <- round(c(error_rate_naive_1, error_rate_naive_2), 3)
tabla_3$Sensitivity_AFF = round(c(confusion_matrix_naive_1$byClass[1], confusion_matrix_naive_2$byClass[1]),3)
tabla_3$Sensitivity_ARR = round(c(confusion_matrix_naive_1$byClass[2,1], confusion_matrix_naive_2$byClass[2,1]),3) 
tabla_3$Sensitivity_CHF = round(c(confusion_matrix_naive_1$byClass[3,1], confusion_matrix_naive_2$byClass[3,1]),3)
tabla_3$Sensitivity_NSR = round(c(confusion_matrix_naive_1$byClass[4,1], confusion_matrix_naive_2$byClass[4,1]),3)

# Imprimimos la tabla
tabla_3 <- tabla_3[order(-tabla_3$Accuracy), ]
kable(tabla_3, caption = "Comparación de Modelos ('Nain Bayes')",  format = "markdown")
```

Al comparar los modelos en la tabla anterior  `r if(confusion_matrix_naive_1$overall["Accuracy"] > confusion_matrix_naive_2$overall["Accuracy"]){"observamos que el modelo obtenido sin activar laplace tiene una mayor precisión"}` `r if(confusion_matrix_naive_1$overall["Accuracy"] < confusion_matrix_naive_2$overall["Accuracy"]){"observamos que el modelo obtenido activando laplace tiene una mayor precisión"}` `r if(confusion_matrix_naive_1$overall["Accuracy"] == confusion_matrix_naive_2$overall["Accuracy"]){"observamos que ambos modelos tienen la misma precisión"}`, `r if(confusion_matrix_naive_1$overall["Kappa"] > confusion_matrix_naive_2$overall["Kappa"]){"el valor kappa también es superior en el modelo sin laplace"}` `r if(confusion_matrix_naive_1$overall["Kappa"] < confusion_matrix_naive_2$overall["Kappa"]){"el valor kappa  es superior en el modelo con laplace"}` `r if(confusion_matrix_naive_1$overall["Kappa"] == confusion_matrix_naive_2$overall["Kappa"]){"que ambos modelos tienen el mismo valor de kappa"}` y `r if(tabla_3$Error_rate[1] > tabla_3$Error_rate[2]){"el valor de error es mayor en el modelo sin laplace"}` `r if(tabla_3$Error_rate[1] < tabla_3$Error_rate[2]){"el valor de error es superior en el modelo con laplace"}` `r if(tabla_3$Error_rate[1] == tabla_3$Error_rate[2]){"que ambos modelos tienen el mismo valor de error"}`.

Los valores de sensibilidad para el modelo con mejor precisión son `r tabla_3$Sensitivity_AFF[1]*100`% para la clase AFF, `r tabla_3$Sensitivity_ARR[1]*100`% para la clase ARR, `r tabla_3$Sensitivity_CHF[1]*100`% para la clase CHF y `r tabla_3$Sensitivity_NSR[1]*100`% para la última clase NSR.

Si ambos modelos son muy similares seleccionaremos el modelo más sencillo.


## Artificial Neural Network

Implementamos el algoritmo **Artificial Neural Network** con el paquete *`keras`* [@keras]. Exploraremos dos tipos de modelos: uno con una única capa oculta con 15 nodos, a la que añadiremos un dropout del 20% para evitar el sobreajuste, y una capa de salida con 4 nodos coincidiendo con nuestras clases de estudio. El segundo modelo contendrá dos capas ocultas con 15 y 35 nodos, a las que se añadirá una capa de dropout del 20%, junto con la capa de salida. Ambos modelos se compilarán y entrenarán, mostrando las gráficas de pérdida y precisión. Por último, visualizaremos las matrices de confusión con el paquete *`caret`*. 

Antes de implementar el modelo, realizaremos un ajuste en nuestros datos de entrenamiento y prueba. Convertiremos nuestras etiquetas a valores numéricos para poder ejecutar el modelo. En la siguiente tabla, observamos la equivalencia de la transformación de las clases de estudio:
 
| Clases     | Clases_ann |
|------------|------------|
| AFF        |       0    |
| ARR        |       1    |
| CHF        |       2    |
| NSR        |       3    |


```{r ajust_data}
# Ajustamos los datos de entramiento y prueba para adaptarlos al algoritmo ann. 
train_data_ann <- as.matrix(train_data[,-1])
test_data_ann <- as.matrix(test_data[,-1])

# Transformamos nuestras clases a valores numericos, del 0 al 3. 
train_label_ann <- as.numeric(factor(train_label)) - 1
test_label_ann <- as.numeric(factor(test_label)) - 1
```

```{r model_onelayer}
set.seed(seed)

# Modelo ann con una capa oculta. 
modelo_ann_1 <- keras_model_sequential() %>%
  # Agregamos una capa densa al modelo con 15 nodos. Establecemos la activacion 'relu' para introducir no linealidades en el modelo.
  layer_dense(units = 15, activation = 'relu', input_shape = ncol(train_data_ann)) %>%
  # Capa de Dropout desactivando aleatoriamente el 20% de las neuronas para prevenir el sobreajuste.
  layer_dropout(rate = 0.2) %>%
  #Capa de salida, usamos la función de activación 'softmax'.
  layer_dense(units = 4, activation = 'softmax')
summary(modelo_ann_1)
```

```{r compile_training_1}
set.seed(seed)

# Compilamos el modelo_ann_1:
modelo_ann_1 %>% compile(
  loss = 'sparse_categorical_crossentropy',  
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)
# Entrenamos el model0_ann_1 y almacenamos la información en la variable "history__ann_1":
history_ann_1 <- modelo_ann_1 %>% fit(
  x = train_data_ann,
  y = train_label_ann,
  epochs = 50, # Establecemos las épocas de interacciones del entrenamiento a 50.
  validation_split = 0.2, # 20% de los datos se usarán como datos de validación. 
  verbose = 2 # Nivel de detalle durante el entrenamiento.
)
plot(history_ann_1) # Mostramos los gáficos de perdida y precisión. 
```

```{r pred_cmatrx_1}
set.seed(seed)
predicciones_ann_1 <- predict(modelo_ann_1, test_data_ann)
y_pred_ann_1 <- as.factor(max.col(predicciones_ann_1) - 1)
matriz_confusion_ann_1 <- confusionMatrix(y_pred_ann_1, as.factor(test_label_ann))
matriz_confusion_ann_1
```

```{r model_twolayers}
# Modelo ann con dos capas ocultas. 
modelo_ann_2 <- keras_model_sequential() %>%
  layer_dense(units = 25, activation = 'relu', input_shape = ncol(train_data_ann)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 15, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 4, activation = 'softmax')
summary(modelo_ann_2)
```

```{r compile_training_2}
set.seed(seed)

# Compilamos el modelo_ann_2:
modelo_ann_2 %>% keras::compile(
  loss = 'sparse_categorical_crossentropy',  
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)
# Entrenamos el modelo_ann_2 y almacenamos la información en la variable "history__ann_2":
history_ann_2 <- modelo_ann_2 %>% fit(
  x = train_data_ann,
  y = train_label_ann,
  epochs = 50,
  validation_split = 0.2,
  verbose = 2
)
plot(history_ann_2)
```

```{r pred_cmatrx_2}
set.seed(seed)
predicciones_ann_2 <- predict(modelo_ann_2, test_data_ann)
y_pred_ann_2 <- as.factor(max.col(predicciones_ann_2) - 1)
matriz_confusion_ann_2 <- confusionMatrix(y_pred_ann_2, as.factor(test_label_ann))
matriz_confusion_ann_2
```

```{r tabla_4_ann, echo=FALSE}
# Mostramos los resultados de los modelo en la siguiente tabla.
Accuracy_val = round(c(matriz_confusion_ann_1$overall["Accuracy"], matriz_confusion_ann_2$overall["Accuracy"]),3)

tabla_4 <- data.frame(
  Modelo = c("ANN One layer", "ANN Two layers"),
  Accuracy = Accuracy_val,
  Kappa = round(c(matriz_confusion_ann_1$overall["Kappa"], matriz_confusion_ann_2$overall["Kappa"]),3),
  Error_rate = (1 - Accuracy_val),
  Sensitivity_AFF = round(c(matriz_confusion_ann_1$byClass[1], matriz_confusion_ann_2$byClass[1]),3),
  Sensitivity_ARR = round(c(matriz_confusion_ann_1$byClass[2,1], matriz_confusion_ann_2$byClass[2,1]),3),  
  Sensitivity_CHF = round(c(matriz_confusion_ann_1$byClass[3,1], matriz_confusion_ann_2$byClass[3,1]),3),
  Sensitivity_NSR = round(c(matriz_confusion_ann_1$byClass[4,1], matriz_confusion_ann_2$byClass[4,1]),3))

tabla_4 <- tabla_4[order(-tabla_4$Accuracy), ]
kable(tabla_4, caption = "Comparación de Modelos ('ANN')", format = "markdown")
```

Al comparar los modelos `r if(matriz_confusion_ann_1$overall["Accuracy"] > matriz_confusion_ann_2$overall["Accuracy"]){"observamos que el modelo obtenido aplicando solo una capa oculta tiene una mayor precisión"}` `r if(matriz_confusion_ann_1$overall["Accuracy"] < matriz_confusion_ann_2$overall["Accuracy"]){"observamos que el modelo obtenido con dos capas ocultas tiene una mayor precisión"}` `r if(matriz_confusion_ann_1$overall["Accuracy"] == matriz_confusion_ann_2$overall["Accuracy"]){"observamos que ambos modelos tienen la misma precisión"}`, `r if(matriz_confusion_ann_1$overall["Kappa"] > matriz_confusion_ann_2$overall["Kappa"]){"el valor kappa también es superior en el modelo con una capa oculta"}` `r if(matriz_confusion_ann_1$overall["Kappa"] < matriz_confusion_ann_2$overall["Kappa"]){"el valor kappa  es superior en el modelo con dos capas ocultas"}` `r if(matriz_confusion_ann_1$overall["Kappa"] == matriz_confusion_ann_2$overall["Kappa"]){"que ambos modelos tienen el mismo valor de kappa"}` y `r if(tabla_4$Error_rate[1] > tabla_4$Error_rate[2]){"el valor de error es mayor en el modelo con una capa"}` `r if(tabla_4$Error_rate[1] < tabla_4$Error_rate[2]){"el valor de error es superior en el modelo con dos capas"}` `r if(tabla_4$Error_rate[1] == tabla_4$Error_rate[2]){"que ambos modelos tienen el mismo valor de error"}`.

Los valores de sensibilidad para el modelo con mejor precisión son `r tabla_4$Sensitivity_AFF[1]*100`% para la clase AFF, `r tabla_4$Sensitivity_ARR[1]*100`% para la clase ARR, `r tabla_4$Sensitivity_CHF[1]*100`% para la clase CHF y `r tabla_4$Sensitivity_NSR[1]*100`% para la última clase NSR.

Si nuestros modelos son muy similares, seguiremos el principio de parsimonia y elegiremos el modelo mas sencillo. 

## Support Vector Machine

Para implementar el algoritmo **upport Vector Machine (SVM)** con kernel lineal y RBF usamos la funcion `ksvm` del paquete *'kernlab'* [@kernlab]. Para el kernel lineal usamos la opción **"vanilladot"** y **"rbfdot"** para RBF. Luego mostramos las matrices de confusión de cada modelo con la función `confusionMatrix()`del paquete *'caret'*.

```{r SVM}
set.seed(seed)

# Convertimos las clases a tipo factor para poder implementar el algoritmo SVM.
train_data$ECG_signal <- as.factor(train_data$ECG_signal)
test_data$ECG_signal <- as.factor(test_data$ECG_signal)

# Modelo SVM con kernel lineal. 
svm_linear_model <- kernlab::ksvm(ECG_signal ~ ., data = train_data, kernel = "vanilladot")
svm_linear_model
predictions_linear <- predict(svm_linear_model, newdata = test_data)

# Modelo SVM con kernel rbf>
svm_rbf_model <- kernlab::ksvm(ECG_signal ~ ., data = train_data, kernel = "rbfdot")
svm_rbf_model
predictions_rbf <- predict(svm_rbf_model, newdata = test_data)

#Matrices de confusión para los modelos. 
linear_model <- caret::confusionMatrix(predictions_linear, test_data$ECG_signal)
rbf_model <- caret::confusionMatrix(predictions_rbf, test_data$ECG_signal)
# Mostramos los datos. 
linear_model
rbf_model
```

```{r tabla_5_SVM, echo=FALSE}
# Mostramos los resultados de los modelo en la siguiente tabla.
Accuracy_val = round(c(linear_model$overall["Accuracy"], rbf_model$overall["Accuracy"]),3)

tabla_5 <- data.frame(
  Modelo = c("SVM Lineal", "SVM gaussiano"),
  Accuracy = Accuracy_val,
  Kappa = round(c(linear_model$overall["Kappa"], rbf_model$overall["Kappa"]),3),
  Error_rate = (1 - Accuracy_val),
  Sensitivity_AFF = round(c(linear_model$byClass[1], rbf_model$byClass[1]),3),
  Sensitivity_ARR = round(c(linear_model$byClass[2,1], rbf_model$byClass[2,1]),3),  
  Sensitivity_CHF = round(c(linear_model$byClass[3,1], rbf_model$byClass[3,1]),3),
  Sensitivity_NSR = round(c(linear_model$byClass[4,1], rbf_model$byClass[4,1]),3))

tabla_5 <- tabla_5[order(-tabla_5$Accuracy), ]
kable(tabla_5, caption = "Comparación de Modelos ('SVM')", format = "markdown")
```

Al comparar los modelos de la tabla anterior `r if(linear_model$overall["Accuracy"] > rbf_model$overall["Accuracy"]){"observamos que el modelo obtenido con SVM lineal tiene una mayor precisión"}` `r if(linear_model$overall["Accuracy"] < rbf_model$overall["Accuracy"]){"observamos que el modelo obtenido con SVM RBF tiene una mayor precisión"}` `r if(linear_model$overall["Accuracy"] == rbf_model$overall["Accuracy"]){"observamos que ambos modelos tienen la misma precisión"}`, `r if(linear_model$overall["Kappa"] > rbf_model$overall["Kappa"]){"el valor kappa también es superior en el modelo lineal"}` `r if(linear_model$overall["Kappa"] < rbf_model$overall["Kappa"]){"el valor kappa  es superior en el modelo radial"}` `r if(linear_model$overall["Kappa"] == rbf_model$overall["Kappa"]){"que ambos modelos tienen el mismo valor de kappa"}` y `r if(tabla_5$Error_rate[1] > tabla_5$Error_rate[2]){"el valor de error es menor en el modelo lineal"}` `r if(tabla_5$Error_rate[1] < tabla_5$Error_rate[2]){"el valor de error es superior en el modelo radial"}` `r if(tabla_5$Error_rate[1] == tabla_5$Error_rate[2]){"que ambos modelos tienen el mismo valor de error"}`.

Los valores de sensibilidad para el modelo con mejor precisión son `r tabla_5$Sensitivity_AFF[1]*100`% para la clase AFF, `r tabla_5$Sensitivity_ARR[1]*100`% para la clase ARR, `r tabla_5$Sensitivity_CHF[1]*100`% para la clase CHF y `r tabla_5$Sensitivity_NSR[1]*100`% para la última clase NSR.

Si ambos modelos son muy similares, optaremos por el modelo más sencillo.

## Árbol de Clasificación

En el siguiente apartado, implementaremos el algoritmo de **Árbol de clasificación** con el paquete *`C50`* [@C50] y la función `C5.0()`. En este caso, exploraremos la implementación de dos modelos: uno sin activar la opción de boosting y otro modelo activando el boosting usando un valor de *trials* igual a 10. También incluiremos las figuras que muestren de forma visual el árbol de clasificación. Por ultimo, se mostrará una tabla resumen de las matrices de confusión de cada modelo. 

```{r treeC_1}
set.seed(seed)
# Modelo sin boosting.
model_tree_1 <- C5.0(train_data[,-1], train_data$ECG_signal, trials = 1) 
summary(model_tree_1)
plot(model_tree_1)
```

```{r pred_treeC_1}
set.seed(seed)
pred_tree_1 <- predict(model_tree_1, test_data)
matriz_confusion_tree_1 <- confusionMatrix(pred_tree_1, test_data$ECG_signal)
matriz_confusion_tree_1
```

```{r treeC_2}
set.seed(seed)
# Modelo aplicando boosting (trials = 10).
model_tree_2 <- C5.0(train_data[,-1], train_data$ECG_signal, trials = 10) 
summary(model_tree_2)
plot(model_tree_2)
```

```{r pred_treeC_2}
set.seed(seed)
pred_tree_2 <- predict(model_tree_2, test_data)
matriz_confusion_tree_2 <- confusionMatrix(pred_tree_2, test_data$ECG_signal)
matriz_confusion_tree_2
```

```{r tabla_6_tree, echo=FALSE}
# Mostramos los resultados de los modelo en la siguiente tabla.
Accuracy_val = round(c(matriz_confusion_tree_1$overall["Accuracy"], matriz_confusion_tree_2$overall["Accuracy"]),3)

tabla_6 <- data.frame(
  Modelo = c("Tree without boosting", "Tree with boosting"),
  Accuracy = Accuracy_val,
  Kappa = round(c(matriz_confusion_tree_1$overall["Kappa"], matriz_confusion_tree_2$overall["Kappa"]),3),
  Error_rate = (1 - Accuracy_val),
  Sensitivity_AFF = round(c(matriz_confusion_tree_1$byClass[1], matriz_confusion_tree_2$byClass[1]),3),
  Sensitivity_ARR = round(c(matriz_confusion_tree_1$byClass[2,1], matriz_confusion_tree_2$byClass[2,1]),3),  
  Sensitivity_CHF = round(c(matriz_confusion_tree_1$byClass[3,1], matriz_confusion_tree_2$byClass[3,1]),3),
  Sensitivity_NSR = round(c(matriz_confusion_tree_1$byClass[4,1], matriz_confusion_tree_2$byClass[4,1]),3))

tabla_6 <- tabla_6[order(-tabla_6$Accuracy), ]
kable(tabla_6, caption = "Comparación de Modelos ('Árbol de clasificación')", format = "markdown")
```

Si comparamos ambos modelos `r if(matriz_confusion_tree_1$overall["Accuracy"] > matriz_confusion_tree_2$overall["Accuracy"]){"observamos que el modelo obtenido sin activar boosting tiene una mayor precisión"}` `r if(matriz_confusion_tree_1$overall["Accuracy"] < matriz_confusion_tree_2$overall["Accuracy"]){"observamos que el modelo obtenido activando boosting tiene una mayor precisión"}` `r if(matriz_confusion_tree_1$overall["Accuracy"] == matriz_confusion_tree_2$overall["Accuracy"]){"observamos que ambos modelos tienen la misma precisión"}`, `r if(matriz_confusion_tree_1$overall["Kappa"] > matriz_confusion_tree_2$overall["Kappa"]){"el valor kappa también es superior en el modelo sin boosting"}` `r if(matriz_confusion_tree_1$overall["Kappa"] < matriz_confusion_tree_2$overall["Kappa"]){"el valor kappa  es superior en el modelo con boosting"}` `r if(matriz_confusion_tree_1$overall["Kappa"] == matriz_confusion_tree_2$overall["Kappa"]){"que ambos modelos tienen el mismo valor de kappa"}` y `r if(tabla_6$Error_rate[1] > tabla_6$Error_rate[2]){"el valor de error es menor en el modelo sin boosting"}` `r if(tabla_6$Error_rate[1] < tabla_6$Error_rate[2]){"el valor de error es superior en el modelo con boosting"}` `r if(tabla_6$Error_rate[1] == tabla_6$Error_rate[2]){"que ambos modelos tienen el mismo valor de error"}`.

Los valores de sensibilidad para el modelo con mejor precisión son `r tabla_6$Sensitivity_AFF[1]*100`% para la clase AFF, `r tabla_6$Sensitivity_ARR[1]*100`% para la clase ARR, `r tabla_6$Sensitivity_CHF[1]*100`% para la clase CHF y `r tabla_6$Sensitivity_NSR[1]*100`% para la última clase NSR.

Si las características de los modelos de árbol de clasificación son similares, optaremos por elegir la opción más sencilla.

## Random Forest

En este último apartado implementaremos el último algoritmo de la práctica evaluable. Se trata del algoritmo **Random Forest**. Para implementar los dos últimos modelos, usaremos la función `randomForest` del paquete *`randomForest`* [@randomForest]. Se presentará de forma gráfica la evolución del error de nuestras clases a medida que se construyen los árboles. Además, se presentará una tabla con las métricas de ambos modelos derivadas de las matrices de confusión.

```{r rf_1}
set.seed(seed)

modelo_rforest_1 <- randomForest(ECG_signal ~ ., data = train_data, ntree = 100)
modelo_rforest_1
plot(modelo_rforest_1)
```

```{r pred_rf_1}
set.seed(seed)
pred_rforest_1 <- predict(modelo_rforest_1, newdata = test_data)
matriz_confusion_rforest_1 <- confusionMatrix(pred_rforest_1 , test_data$ECG_signal)
matriz_confusion_rforest_1
```

```{r rf_2}
set.seed(seed)

modelo_rforest_2 <- randomForest(ECG_signal ~ ., data = train_data, ntree = 200)
modelo_rforest_2
plot(modelo_rforest_2)
```

```{r pred_rf_2}
set.seed(seed)
pred_rforest_2 <- predict(modelo_rforest_2, newdata = test_data)
matriz_confusion_rforest_2 <- confusionMatrix(pred_rforest_2 , test_data$ECG_signal)
matriz_confusion_rforest_2
```

```{r tabla_7_randomf, echo=FALSE}
# Mostramos los resultados de los modelo en la siguiente tabla.
Accuracy_val = round(c(matriz_confusion_rforest_1$overall["Accuracy"], matriz_confusion_rforest_2$overall["Accuracy"]),3)

tabla_7 <- data.frame(
  Modelo = c("rForest n=100", "rForest n=200"),
  Accuracy = Accuracy_val,
  Kappa = round(c(matriz_confusion_rforest_1$overall["Kappa"], matriz_confusion_rforest_2$overall["Kappa"]),3),
  Error_rate = (1 - Accuracy_val),
  Sensitivity_AFF = round(c(matriz_confusion_rforest_1$byClass[1], matriz_confusion_rforest_2$byClass[1]),3),
  Sensitivity_ARR = round(c(matriz_confusion_rforest_1$byClass[2,1], matriz_confusion_rforest_2$byClass[2,1]),3),  
  Sensitivity_CHF = round(c(matriz_confusion_rforest_1$byClass[3,1], matriz_confusion_rforest_2$byClass[3,1]),3),
  Sensitivity_NSR = round(c(matriz_confusion_rforest_1$byClass[4,1], matriz_confusion_rforest_2$byClass[4,1]),3))

tabla_7 <- tabla_7[order(-tabla_7$Accuracy), ]
kable(tabla_7, caption = "Comparación de Modelos ('Ramdon Forest')", format = "markdown")
```

Comparando los dos ultimos modelos `r if(matriz_confusion_rforest_1$overall["Accuracy"] > matriz_confusion_rforest_2$overall["Accuracy"]){"observamos que el modelo obtenido con un número de árboles igual a 100 tiene una mayor precisión"}` `r if(matriz_confusion_rforest_1$overall["Accuracy"] < matriz_confusion_rforest_2$overall["Accuracy"]){"observamos que el modelo obtenido con un número de árboles igual a 200 tiene una mayor precisión"}` `r if(matriz_confusion_rforest_1$overall["Accuracy"] == matriz_confusion_rforest_2$overall["Accuracy"]){"observamos que ambos modelos tienen la misma precisión"}`, `r if(matriz_confusion_rforest_1$overall["Kappa"] > matriz_confusion_rforest_2$overall["Kappa"]){"el valor kappa también es superior en el modelo con un número de árboles igual a 100"}` `r if(matriz_confusion_rforest_1$overall["Kappa"] < matriz_confusion_rforest_2$overall["Kappa"]){"el valor kappa  es superior en el modelo con un número de árboles igual a 200"}` `r if(matriz_confusion_rforest_1$overall["Kappa"] == matriz_confusion_rforest_2$overall["Kappa"]){"que ambos modelos tienen el mismo valor de kappa"}` y `r if(tabla_7$Error_rate[1] > tabla_7$Error_rate[2]){"el valor de error es menor en el modelo con un número de árboles igual a 100"}` `r if(tabla_7$Error_rate[1] < tabla_7$Error_rate[2]){"el valor de error es superior en el modelo con un número de árboles igual a 100"}` `r if(tabla_7$Error_rate[1] == tabla_7$Error_rate[2]){"que ambos modelos tienen el mismo valor de error"}`.

Los valores de sensibilidad para el modelo con mejor precisión son `r tabla_7$Sensitivity_AFF[1]*100`% para la clase AFF, `r tabla_7$Sensitivity_ARR[1]*100`% para la clase ARR, `r tabla_7$Sensitivity_CHF[1]*100`% para la clase CHF y `r tabla_7$Sensitivity_NSR[1]*100`% para la última clase NSR.

Como en los modelos anteriores, si obtenemos métricas muy similares, volveremos a elegir el más sencillo.

# Discusión y Conclusión

```{r tabla_modelos, echo=FALSE}
tabla_modelos <- rbind(tabla_2, tabla_3, tabla_4, tabla_5, tabla_6, tabla_7)
# Ordenamos los modelos por mejor precisión.
tabla_modelos <- tabla_modelos[order(-tabla_modelos$Accuracy), ]
modelos_seleccionados <- subset(tabla_modelos, Accuracy >= 0.95)
```

En la siguiente tabla se presentan todos los modelos implementados en esta actividad, tenemos `r nrow(tabla_modelos)` modelos. Se incluyen las métricas de precisión (Accuracy), el valor Kappa, el ratio de error (Error_rate) y la sensibilidad de cada clase de estudio. Cada modelo se ha ordenado por su valor de precisión de mayor a menor. 

```{r prints, echo=FALSE}
kable(tabla_modelos, caption = "Modelos", format = "markdown")
# Grafico de las métricas de los modelos. 
ggplot(tabla_modelos, aes(x = Modelo, y = Accuracy, group = 1)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") +
  geom_hline(yintercept = tabla_modelos$Accuracy[1], linetype = "solid", color = "red") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Comparación de Precisión entre Modelos", x = "Modelo", y = "Precisión")
```

Para la clasificación de dolencias cardiacas hemos explorado distintos tipos de algoritmos. En la tabla anterior, podemos comparar las métricas de cada uno. Los mejores tres modelos se corresponden, de más preciso a menos, con `r tabla_modelos$Modelo[1]`, `r tabla_modelos$Modelo[2]` y `r tabla_modelos$Modelo[3]`. La precisión de los modelos varía desde `r tabla_modelos$Accuracy[1]*100`% hasta `r tabla_modelos$Accuracy[15]*100`%, lo que nos da una diferencia de un `r ((tabla_modelos$Accuracy[1])-(tabla_modelos$Accuracy[15]))*100`% entre el primero y el último. Sin embargo, entre el primer modelo y el segundo tenemos una diferencia del `r ((tabla_modelos$Accuracy[1])-(tabla_modelos$Accuracy[2]))*100`% y una diferencia con el tercero del `r ((tabla_modelos$Accuracy[1])-(tabla_modelos$Accuracy[3]))*100`%.
 
El modelo `r tabla_modelos$Modelo[1]` tiene las mejores métricas en cuanto a precisión (`r tabla_modelos$Accuracy[1]*100`%), valor Kappa (`r tabla_modelos$Kappa[1]*100`%), y ratio de error `r tabla_modelos$Error_rate[1]*100`%. Los valores de sensibilidad para el modelo `r tabla_modelos$Modelo[1]` con la mejor precisión (`r tabla_modelos$Accuracy[1]*100`%) son `r tabla_modelos$Sensitivity_AFF[1]*100`% para la clase AFF, `r tabla_modelos$Sensitivity_ARR[1]*100`% para la clase ARR, `r tabla_modelos$Sensitivity_CHF[1]*100`% para la clase CHF, y `r tabla_modelos$Sensitivity_NSR[1]*100`% para la última clase NSR.

A la hora de elegir un modelo, tenemos que tener en cuenta las métricas. Aun así, si nuestros modelos muestran valores muy similares, podemos establecer un valor de corte (cutoff). En nuestro caso, vamos a seleccionar los modelos que tengan una precisión del 95% o más:

```{r select_model, echo=FALSE}
kable(modelos_seleccionados$Modelo, caption = "Modelos Seleccionados", format = "markdown")
```

De los modelos seleccionados, los dos primeros destacan por sus métricas. Como resultado final de esta práctica evaluable, elegimos estos modelos, `r tabla_modelos$Modelo[1]` y `r tabla_modelos$Modelo[2]`, como los mejores para clasificar las dolencias cardiacas. Si los modelos coinciden en que son del mismo tipo de algoritmo, nos quedaremos únicamente con el más sencillo. De forma adicional, podríamos incluir el tercer tipo de modelo (`r tabla_modelos$Modelo[3]`).

\newpage

# Referencias